# 1. 운영체제의 큰 그림

운영체제는 매우 많은 종류와 다양한 기능이 있지만, 핵심적인 기능은 모두 비슷하다.

이 핵심적인 기능을 담당하는 부분을 **커널(Kernel)**이라고 부른다.

운영체제의 핵심 기능은 **자원 할당 및 관리**와 **프로세스 및 스레드 관리** 두 가지가 있다.

이 두 가지 핵심 기능을 정리하고, 프로그램들이 운영체제의 기능을 제공받는 방법을 살펴보도록 하자.

## 운영체제의 역할

운영체제의 역할은 크게 4개로 나누어 볼 수 있다.

1. **CPU 관리**
   * 어떤 프로세스에 CPU를 할당하고, 얼마나 오랫동안 사용할지 결정한다.
2. **메모리 관리**
   * 프로세스에 메모리 공간을 할당하고, 실제 메모리보다 큰 공간을 사용할 수 있도록 관리한다.
3. **파일/디렉토리 관리**
   * 파일과 디렉토리를 생성, 삭제, 접근하고 관리하는 체계를 제공한다.
4. **프로세스 및 스레드 관리**
   * 실행 중인 프로그램(프로세스)과 그 내부의 실행 흐름(스레드)을 생성하고 관리한다.

## 시스템 콜과 이중 모드

컴퓨터 메모리는 크게 커널 영역(Kernel Space)과 사용자 영역(User Space)으로 나뉜다. 

운영체제의 핵심 코드, 즉 커널은 커널 영역에 상주하며 시스템의 모든 자원에 접근할 수 있는 특별한 권한을 가진다. 

반면, 우리가 일반적으로 사용하는 응용 프로그램(웹 브라우저, 게임 등)은 사용자 영역에서 실행되며, 하드웨어나 중요한 시스템 자원에 직접 접근하는 것이 엄격히 제한된다. 

이는 시스템의 안정성과 보안을 유지하기 위한 필수적인 조치이다.

하지만 응용 프로그램이 파일을 읽거나, 네트워크 통신을 하거나, 새로운 프로세스를 생성하는 등 작업을 수행하려면 반드시 운영체제의 도움이 필요하다. 

사용자 영역의 응용 프로그램이 어떻게 커널 영역의 기능을 안전하게 사용할 수 있는가?

여기서 등장하는 개념이 바로 **시스템 콜(System Call)** 과 **이중 모드(Dual Mode)** 이다.

### 이중 모드 (Dual Mode)
   * CPU는 현재 실행되는 코드가 운영체제 커널인지, 아니면 일반 응용 프로그램인지 구분하기 위해 최소 두 가지 모드, 즉 커널 모드(Kernel Mode)와 사용자 모드(User Mode)로 작동한다.
   * **커널 모드**: 운영체제 커널 코드가 실행되는 모드이며, 모든 하드웨어 접근과 시스템 명령 실행이 가능하다.
   * **사용자 모드**: 일반 응용 프로그램이 실행되는 모드이며, 직접적인 하드웨어 접근이나 중요한 시스템 설정 변경이 불가능하다.

### 시스템 콜 (System Call)
   * 사용자 모드에서 실행되는 응용 프로그램이 커널 모드의 운영체제 서비스를 요청하는 공식적인 통로(인터페이스)이다. 
   * 응용 프로그램은 특정 작업을 수행하기 위해 해당하는 시스템 콜을 호출한다. 
   * 예를 들어, 파일을 열기 위해 open(), 새로운 프로세스를 만들기 위해 fork()와 같은 시스템 콜을 사용한다.

### 시스템 콜의 작동 과정

1. **요청 (Request)**: 사용자 영역의 응용 프로그램이 파일 읽기 같은 OS 서비스가 필요하면, 시스템 콜을 호출한다.
2. **소프트웨어 인터럽트 (Trap)**: 시스템 콜 호출은 일종의 '소프트웨어 인터럽트'를 발생시킨다.
3. **모드 전환 (Mode Switch)**: CPU는 이 인터럽트를 감지하고, 현재 실행 모드를 사용자 모드에서 커널 모드로 전환한다. 동시에 프로그램 실행 제어를 운영체제 커널 내의 해당 시스템 콜 처리 루틴으로 넘긴다.
4. **커널 작업 수행 (Kernel Execution)**: 커널 모드로 전환된 CPU는 요청받은 운영체제 서비스(예: 디스크에서 파일 읽기)를 안전하게 수행한다.
5. **결과 반환 및 모드 복귀 (Return & Mode Switch)**: 작업이 완료되면, CPU는 다시 커널 모드에서 사용자 모드로 전환하고, 시스템 콜의 결과를 응용 프로그램에게 돌려준다. 응용 프로그램은 시스템 콜 호출 이후의 코드를 계속 실행한다.
   
이처럼 시스템 콜과 이중 모드는 응용 프로그램이 필요한 운영체제 기능을 안전하고 통제된 방식으로 사용할 수 있도록 하는 핵심 메커니즘이다. 

간단해 보이는 "hello world" 출력 프로그램조차 내부적으로 수많은 시스템 콜을 호출하며 운영체제와 상호작용한다. 

이는 운영체제가 어떻게 응용 프로그램에게 안정적인 실행 환경을 제공하고 시스템 자원을 보호하는지를 보여주는 중요한 원리이다.

<br/><br/>





# 2. 프로세스와 스레드

프로세스는 실행 방식과 사용자와의 상호작용 여부에 따라 **포그라운드 프로세스(Foreground Process)** 와 **백그라운드 프로세스(Background Process)** 로 나눌 수 있다. 

- 포그라운드 프로세스
  - 사용자가 직접 입력하고 결과를 확인하는 등 상호작용하는 프로세스

- 백그라운드 프로세스
  - 사용자 인터페이스 없이 뒤에서 조용히 실행되는 프로세스이다. 
  - 특히, 시스템 운영을 위해 백그라운드에서 특정 기능을 수행하며 계속 실행되는 프로세스를 유닉스 계열에서는 **데몬(Daemon)**, 윈도우에서는 **서비스(Service)** 라고 부른다.

![프로세스 구조](./img/bsh_1.png)

각 프로세스는 운영체제에 의해 관리되며 고유한 메모리 구조를 가진다. 

이 메모리 공간은 크게 **커널 영역(Kernel Area)** 과 **사용자 영역(User Area)** 으로 구분된다. 

**커널 영역**에는 운영체제가 해당 프로세스를 관리하기 위한 핵심 정보, 즉 **프로세스 제어 블록(PCB, Process Control Block)** 이 저장된다. 

**사용자 영역**은 실제 프로그램 코드가 실행되는 공간으로, 일반적으로 다음과 같은 세부 영역으로 나뉜다:

- **코드 영역(Code Area)**
  - 실행할 프로그램의 코드가 저장되는 영역이다.
  - 텍스트 영역이라고도 불린다.
  - 쓰기가 금지된 읽기 전용 공간이다.
- **데이터 영역(Data Area)**
  - 전역 변수, 정적(static) 변수 등 프로그램 실행 중에 사용되는 데이터가 저장되는 영역이다.
  - 정적 할당 영역이라고도 불린다.
- **힙 영역(Heap Area)**
  - 프로그래머가 동적으로 할당하고 해제하는 메모리 공간이다 (예: `malloc`, `new` 연산자로 할당).
  - 힙 영역에 할당된 메모리는 언젠간 해당 영역을 반드시 해제해야 한다.
  - 그렇지 않으면 메모리 누수가 발생할 수 있다.
  - 프로그래밍 언어들은 사용되지 않는 힙 메모리 영역을 자동으로 해제해주는 가비지 컬렉션 기능을 제공한다.
- **스택 영역(Stack Area)**
    - 함수 호출 시 생성되는 지역 변수, 매개변수, 함수 반환 주소 등이 임시로 저장되는 영역이다. 
    - 함수 호출이 완료되면 해당 정보는 스택에서 제거된다.

---

### PCB와 문맥 교환

운영체제는 동시에 여러 프로세스를 처리해야 하므로, 각 프로세스의 상태와 정보를 관리해야 한다. 

이를 위해 사용하는 자료구조가 바로 **프로세스 제어 블록(PCB, Process Control Block)** 이다. 

PCB는 커널 영역에 생성되며, 프로세스를 식별하고 관리하는 데 필요한 모든 정보를 담고 있다.

#### PCB 주요 정보
*   프로세스 식별자 (PID)
*   프로세스 상태 (생성, 준비, 실행, 대기, 종료 등)
*   프로그램 카운터 (다음에 실행할 명령어 주소)
*   CPU 레지스터 값 (문맥 교환 시 저장 및 복원될 레지스터 값들)
*   CPU 스케줄링 정보 (우선순위, 스케줄링 큐 포인터 등)
*   메모리 관리 정보 (페이지 테이블, 세그먼트 테이블 등)
*   입출력 상태 정보 (할당된 입출력 장치, 열린 파일 목록 등)

아래는 리눅스 커널 코드의 실제 PCB 구조체(`task_struct`)의 일부 예시이다.

```c++
struct task_struct {
    pid_t pid;                       // PID
    Int prio;                        // 스케줄링 관련 정보
    unsigned int_state;              // 프로세스 상태 관련 정보
    struct mm_struct * mm;           // 메모리 관련 정보
    void * stack;                    // 스택 관련 정보
    struct files_struct * files;     // 파일 관련 정보
}
```

운영체제는 이러한 PCB들을 **프로세스 테이블** 형태로 관리한다.

![](./img/bsh_2.png)

---

<br/>

일반적으로 메모리에 적재된 프로세스들은 **한정된 시간 동안 번갈아 가며 실행**된다.
*   **'프로세스가 실행된다'** 는 것은 운영체제에 의해 **CPU 자원을 할당받았다**는 의미이다.
*   CPU는 프로세스의 명령어를 실행하고, 운영체제는 CPU 자원을 관리 및 할당한다.
*   따라서 여러 프로세스가 번갈아 실행된다는 것은, 각 프로세스가 **운영체제로부터 CPU 자원을 번갈아 할당받아 이용한다**는 뜻이다.
*   이처럼 여러 프로세스를 번갈아 실행하기 위해, 운영체제는 **타이머 인터럽트(Timer Interrupt)** 를 사용한다.

타이머 인터럽트나 입출력 작업 등으로 인해 실행 중인 프로세스를 잠시 멈추고 다른 프로세스를 실행해야 할 때, 운영체제는 현재 프로세스의 상태를 저장해야 나중에 이어서 실행할 수 있다. 

이때 필요한 정보들이 **문맥**과 **문맥 교환**이다.

*   **문맥 (Context)**
    *   CPU가 프로세스를 전환할 때, 현재 프로세스의 실행 상태를 나중에 복원하기 위해 저장해야 하는 정보의 집합이다.
    *   주요 내용으로는 CPU 레지스터 값(프로그램 카운터, 스택 포인터 등), 프로세스 상태, 메모리 관리 정보 등이 있다.

![문맥 교환](./img/bsh_4.png)

*   **문맥 교환 (Context Switching)**
    *   현재 실행 중인 프로세스의 문맥을 해당 프로세스의 PCB(프로세스 제어 블록)에 저장하는 과정이다.
    *   다음에 실행할 프로세스의 문맥을 해당 프로세스의 PCB에서 읽어와 CPU 레지스터 등에 복원하는 과정이다.
    *   운영체제가 여러 프로세스를 번갈아 실행하며 멀티태스킹을 가능하게 하는 핵심 메커니즘이다.

![문맥 교환 과정](./img/bsh_5.png)

문맥 교환 과정을 더 자세히 보면 다음과 같다:

1.  **현재 프로세스의 문맥 저장:** 현재 실행 중인 프로세스 P1의 문맥(프로그램 카운터, 레지스터 값 등)을 P1의 PCB에 저장한다.
2.  **다음 프로세스의 문맥 복원:** 다음에 실행할 프로세스 P2의 PCB에서 저장된 문맥을 읽어와 CPU 레지스터 등에 복원한다.
3.  프로세스 P2 실행 재개

#### 주의점
- 이 과정을 통해 여러 프로세스가 CPU를 나누어 사용하는 것처럼 보이지만, **문맥 교환은 그 자체로 CPU 시간을 소모하는 오버헤드 작업이다.** 
- 문맥을 저장하고 복원하는 데 시간이 필요하기 때문이다.
- 따라서 **문맥 교환이 너무 자주 발생하면** 실제 유용한 작업 처리 시간보다 문맥 교환에 소요되는 시간이 많아져 **시스템 전체 성능이 저하**될 수 있다.

---

### 프로세스의 상태

하나의 프로세스는 여러 상태를 거치며 실행된다.

프로세스의 상태는 대표적으로 5가지로 나누어진다.

![프로세스 상태](./img/bsh_6.png)

*   **생성 상태 (New)**
    *   프로세스를 막 생성한 상태이다.
    *   메모리에 적재되어 PCB(프로세스 제어 블록)를 할당받는다.
    *   생성 상태를 거쳐 실행 준비가 완료되면 준비 상태가 되어 CPU 할당을 기다린다.

*   **준비 상태 (Ready)**
    *   CPU만 할당받으면 즉시 실행할 수 있는 상태이다.
    *   아직 자신의 차례가 아니기 때문에 CPU 할당을 기다리고 있다.
    *   준비 상태의 프로세스가 CPU를 할당받아 실행 상태로 전환되는 것을 **디스패치(Dispatch)**라고 한다.

*   **실행 상태 (Running)**
    *   CPU를 할당받아 실행 중인 상태이다.
    *   할당된 시간(타임 슬라이스) 동안 CPU를 사용한다.
    *   만약 할당된 시간을 모두 사용하면 타이머 인터럽트에 의해 다시 준비 상태로 전환된다.
    *   실행 도중 입출력 장치를 사용해야 하면 입출력 작업이 끝날 때까지 기다리는 대기 상태로 전환된다.

*   **대기 상태 (Blocked / Wait)**
    *   프로세스가 입출력 작업을 요청하거나, 즉시 확보할 수 없는 자원을 요청하는 등 특정 이벤트가 발생하기를 기다리는 상태이다.
    *   예를 들어, 입출력 작업을 요청한 경우 해당 작업이 완료될 때까지 대기한다.
    *   대기 상태에 있던 프로세스는 기다리던 이벤트가 완료되면(예: 입출력 작업 완료) 다시 준비 상태가 되어 CPU 할당을 기다린다.

*   **종료 상태 (Terminated)**
    *   프로세스가 종료된 상태이다.
    *   프로세스가 종료되면 운영체제는 해당 프로세스의 PCB와 사용했던 메모리를 정리한다.

## 멀티프로세스와 멀티스레드

하나의 프로그램을 실행할 때, 그 안의 여러 코드를 동시에 실행하고 싶다면 어떻게 해야 할까? 

크게 두 가지 방법이 있다.

---

### 멀티프로세싱 (Multi-processing)

![멀티프로세스](./img/bsh_7.png)

*   각각의 코드를 별도의 **프로세스**로 생성하여 동시에 실행하는 방식이다.
*   **예시:** 웹 브라우저에서 여러 탭을 열 때, 각 탭이 독립적인 프로세스로 동작하는 경우가 많다.
*   **특징:**
    *   각 프로세스는 자신만의 독립적인 메모리 공간과 자원을 가진다 (자원 공유 X).
    *   프로세스들은 서로에게 영향을 주지 않고 독립적으로 실행된다.
    *   각각 고유한 PID(프로세스 ID)를 가진다.

---

### 멀티스레딩 (Multi-threading)

![멀티스레드](./img/bsh_8.png)

*   하나의 프로세스 내에서 여러 개의 **스레드(Thread)** 를 만들어 코드들을 동시에 실행하는 방식이다.
*   **스레드의 구성**
    *   각 스레드는 독립적인 실행 흐름을 위해 고유한 스레드 ID, 프로그램 카운터(PC), 레지스터 값, 스택 영역 등을 가진다.
    *   이를 통해 각 스레드는 다음에 실행할 명령어의 주소와 연산 중 필요한 임시 데이터를 개별적으로 관리할 수 있다.

---

### 멀티프로세스와 멀티스레드의 차이점

가장 핵심적인 차이점은 **자원 공유 여부**이다.

![멀티프로세스와 멀티스레드의 차이점](./img/bsh_9.png)

*   **멀티프로세싱 (서로 다른 프로세스들):**
    *   **자원 공유:** 기본적으로 자원을 공유하지 않는다. 각 프로세스는 완전히 독립적인 메모리 공간을 가진다.
    *   **독립성:** 매우 높다. 한 프로세스의 오류가 다른 프로세스에 영향을 미치지 않는다.
    *   **통신:** 프로세스 간 통신(IPC) 메커니즘이 필요하며, 상대적으로 복잡하다.
    *   **오버헤드:** 프로세스 생성 및 문맥 교환 시 오버헤드가 크다.

*   **멀티스레딩 (같은 프로세스 내 여러 스레드들):**
    *   **자원 공유:** 프로세스의 자원(코드 영역, 데이터 영역, 힙 영역, 열린 파일 등)을 공유한다. (단, 스택 영역은 스레드마다 별도로 가짐)
    *   **독립성:** 낮다. 한 스레드의 오류가 전체 프로세스에 영향을 줄 수 있다.
    *   **통신:** 공유 메모리를 통해 쉽게 데이터를 주고받을 수 있어 통신이 용이하다.
    *   **오버헤드:** 스레드 생성 및 문맥 교환 시 오버헤드가 프로세스보다 작다.

## 프로세스 간 통신

프로세스끼리는 기본적으로 자원을 공유하지 않지만, 프로세스끼리 자원을 공유하고 데이터를 주고 받을 수 있는 방법이 있다.

이를 **프로세스 간 통신(IPC, Inter-Process Communication)** 이라고 한다.

프로세스 간 통신에는 크게 두 가지 방식, **공유 메모리**와 **메시지 전달**이 있다.

---

### 공유 메모리
공유 메모리는 프로세스 간에 공유하는 메모리 영역을 토대로 데이터를 주고 받는 통신 방식이다.

![공유 메모리1](./img/bsh_10.png)

공유 메모리 기반 IPC는 

- 프로세스가 공유하는 메모리 영역을 확보하는 시스템 콜을 기반으로 수행될 수도 있고,
- 프로세스가 공유하는 변수나 파일을 활용할 수도 있다.

![공유 메모리2](./img/bsh_11.png)

#### 특징

- 마치 자신의 메모리 영역을 읽고 쓰는 통신한다.
- 프로세스끼리 데이터를 주고받는 과정에 커널의 개입이 없다(데이터가 커널 영역을 거치지 않는 경우가 많음).
- 메시지 전달 방식보다 통신 속도가 빠르다.
- 데이터 일관성을 해칠 수 있다(경쟁 상태).

---

### 메시지 전달 

메시지 전달은 프로세스 간 주고받을 데이터가 커널을 거쳐 송수신되는 IPC이다.

공유 메모리 방식과는 다르게 메시지를 보내는 수단과 받는 수단이 명확하게 구분되어 있다(send() 시스템콜, receive() 시스템콜).

또한 커널의 도움을 받을 수 있어 경쟁 상태, 동기화 등 문제를 고려할 일이 적다.

메시지 전달 IPC를 위한 대표적인 수단으로 파이프, 시그널, 소켓, 원격 프로시저 호출(RPC) 등이 있다.

#### 파이프

파이프란 단방향 프로세스 간의 통신 도구를 뜻한다.

![파이프 메시지 전달](./img/bsh_12.png)

양방향 통신이 필요한 경우 읽기용 파이프와 쓰기용 파이프 2개를 이용해 통신한다.

#### 시그널

시그널은 프로세스에게 특정 **이벤트**가 발생했음을 알리는 비동기적인 신호이다.

시그널은 IPC만의 개념이 아니기에, '시그널을 적절히 활용해 IPC를 수행할 수 있다' 정도로 이해해야 한다.

다음은 리눅스 운영체제의 대표적인 시그널 예시이다.

![시그널](./img/bsh_13.png)

시그널이 발생하면 프로세스는 인터럽트 처리 과정과 유사하게 작동한다.

1. 하던 일을 중단한다.
2. 시그널 처리를 위한 **시그널 핸들러**를 실행한다.
3. 원래 작업을 재개한다.

파이프 방식과 똑같이 뭔가를 전달해주는 방식은 똑같아 보이지만, 파이프는 **데이터 전달 중심**이고 동기적이라면 시그널은 **이벤트 알림 중심**이고 비동기적인 것에서 차이가 있다고 볼 수 있다.

<br/><br/>





# 3. 동기화와 교착 상태

공유 자원에 접근하는 코드 중 동시에 실행했을 때 문제가 발생할 수 있는 코드를 **임계 구역**이라고 한다.

다음은 프로세스끼리 공유하는 자원을 이용했을 때 임계 구역의 예시이다.

![임계 구역1](./img/bsh_14.png)

- 프로세스 B가 실행된 뒤에 프로세스 A가 실행되는 것은 문제가 될 수 있다.

- 이는 프로세스 B가 아직 쓰이지 않은 메모리를 읽으려 했기 때문이다.

다음은 스레드끼리 공유하는 자원을 이용했을 때의 예시이다.

![임계 구역2](./img/bsh_15.png)

- 공유하는 자원을 서로 수정하려고 하는 상황.
- 먼저 쓰기를 진행했던 스레드 A의 작업 내역은 반영되지 않을 수 있다.

![임계 구역3](./img/bsh_16.png)

- 혹은 위처럼 문맥 교환이 발생하는 경우에도 각 스레드가 파일을 수정하는 코드는 임계 구역이 된다.

위 예시들을 통해 도출해낼 수 있는 정보들은 다음과 같다.

- **경쟁 상태 (Race Condition)**
  - 여러 프로세스나 스레드가 동시에 공유 자원(예: 변수, 파일 등)에 접근하여 값을 변경하려고 할 때, 접근 순서에 따라 결과가 달라지는 예측 불가능한 상태를 말한다.
  - 이는 데이터의 일관성을 깨뜨릴 수 있다.
- **임계 구역 (Critical Section)**
  - 코드 내에서 공유 자원에 접근하는 부분을 의미한다.
  - 경쟁 상태는 바로 이 임계 구역에서 발생할 수 있다.
- **상호 배제 (Mutual Exclusion)**
  - 임계 구역 문제를 해결하기 위한 핵심 원칙이다.
  - 즉, 한 번에 오직 하나의 프로세스 또는 스레드만이 임계 구역에 진입하도록 허용하고, 이미 임계 구역 안에 있는 작업이 끝나기 전까지 다른 작업들은 반드시 기다리도록 강제하는 것이다.


## 동기화 기법

**동기화**란 경쟁 상태를 해결하기 위해 상호 배제를 보장하는 것을 의미한다.

동기화 기법은 **뮤텍스 락**, **세마포**, **모니터** 등이 있다.

---

### 뮤텍스 락

**뮤텍스 락**은 공유 자원에 동시 접근이 불가능하도록 상호 배제를 보장하는 동기화 도구이다.

뮤텍스 락의 원리는 다음과 같이 단순하다.

> 임계 구역에 접근하고자 한다면 반드시 락(lock)을 획득(acquire)해야 한다.
>
> 임계 구역에서의 작업이 끝나면 락을 해제(release)해야 한다.

특이점은, acquire()은 특정 락에 대해 한 번만 호출 가능한 함수이고, 이후 다른 프로세스 및 스레드가 acquire()을 호출하더라도 락을 획득할 수 없다.

![뮤텍스 락](./img/bsh_17.png)

간단한 코드로 표현하면 다음과 같다.

```c++
lock.acquire();
// 임계 구역
lock.release();
```

다음은 경쟁 상태를 해결하는 예시 코드이다. 공유 자원은 최종적으로 0이 되어야 한다.

```java
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

public class MutexExample {

    private static int sharedData = 0;
    private static final Lock lock = new ReentrantLock();

    public static void main(String[] args) {
        System.out.println("초기 값: " + sharedData);

        Thread thread1 = new Thread(new Increment());
        Thread thread2 = new Thread(new Decrement());

        // 스레드 시작
        thread1.start();
        thread2.start();

        try {
            thread1.join();
            thread2.join();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            System.err.println("메인 스레드 대기 중 인터럽트 발생");
            e.printStackTrace();
        }

        System.out.println("최종 값: " + sharedData);
    }

    static class Increment implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 100000; i++) {
                lock.lock(); // --- 임계 구역 시작 (락 획득) ---
                try {
                    sharedData++; // 공유 데이터 증가
                } finally {
                    lock.unlock(); // --- 임계 구역 끝 (락 해제) ---
                }
            }
            System.out.println(Thread.currentThread().getName() + " 완료.");
        }
    }

    static class Decrement implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 100000; i++) {
                lock.lock(); // --- 임계 구역 시작 (락 획득) ---
                try {
                    sharedData--; // 공유 데이터 감소
                } finally {
                    lock.unlock(); // --- 임계 구역 끝 (락 해제) ---
                }
            }
            System.out.println(Thread.currentThread().getName() + " 완료.");
        }
    }
}
```

---

### 세마포

- 뮤텍스 락: 한 번에 하나의 프로세스나 스레드만 공유 자원을 이용해야 하는 상황.
- 세마포: 한 번에 여러 프로세스나 스레드가 특정 자원을 이용하는 상황에서 사용.

세마포는 하나의 변수와 2개의 함수로 구성된다.

- 변수 S: 사용 가능한 공유 자원의 개수를 나타낸다.
- wait(): 임계 구역 진입 전 
- signal(): 임계 구역 진입 후

세마포도 뮤텍스 락과 똑같이 임계 구역 진입 전/후로 wait()과 signal()을 호출하면 된다.

```c++
wait()
// 임계 구역
signal()
```

#### wait() 함수의 작동 방식

1. wait() 함수 호출시 '사용 가능한 공유 자원의 개수'를 나타내는 변수 S를 1 감소시킨다.
2. 변수 S의 값이 0보다 작은지 여부를 확인한다. 
   - S를 1 감소시켰을 때 S>=0 이라면 사용 가능한 공유 자원의 개수가 남아 있었음을 의미한다. 이 경우, wait()를 호출한 프로세스 및 스레드는 임계 구역에 진입한다. 
   - 반대로, S를 1 감소시켰을 때 S<0 이라면 사용 가능한 공유 자원의 개수가 남아 있지 않았음을 의미한다.
3. S<0 이라면 wait()를 호출한 프로세스 및 스레드는 대기 상태로 전환되어 임계 구역에 진입할 수 없게 된다.

```c++
wait() {
    S--;          /* ① */
    if ( S < 0 ) {  /* ② */
        sleep();    /* ③ */
    }
}
```

#### signal() 함수의 작동 방식

1. signal()은 함수 호출 시 가장 먼저 ① '사용 가능한 공유 자원의 개수'를 나타내는 변수 S를 1 증가시킨다.
2. 변수 S의 값이 0 이하인지 확인한다. 
   - S를 1 증가시켰을 때 S가 0보다 크다는 것은 사용 가능한 공유 자원의 개수가 1개 이상 남아 있음을 의미하며, 이 경우는 대기 중인 프로세스/스레드가 없다고 간주할 수 있다. 
   - 반대로 S를 1 증가시켰을 때 0 이하라는 것은 (값을 증가시키기 전에는 음수였을 수 있으므로) 임계 구역에 진입하기 위해 대기하는 프로세스/스레드가 존재함을 의미한다.
3. S <= 0의 경우, 대기 상태로 접어든 프로세스 중 하나를 깨워 준비 상태로 전환한다. (wakeup(p)는 대기 중인 프로세스 p를 깨우는 동작을 나타낸다.)

```c++
signal() {
    S++;           /* ① */
    if ( S <= 0 ) {  /* ② */
        wakeup(p);   /* ③ */
    }
}
```

#### wait()과 signal()이 어떻게 동기화를 가능하게 하는가?

![세마포 동기화](./img/bsh_18.png)

#### 세마포 코드 예시

```java
import java.util.concurrent.Semaphore;

public class Sem {

    static int sharedData = 0; // 공유 데이터 
    static Semaphore semaphore = new Semaphore(1); // 세마포 생성, 공유 자원 1개

    public static void main(String[] args) {
        Thread thread1 = new Thread(new Increment());
        Thread thread2 = new Thread(new Decrement());

        thread1.start(); // 첫 번째 스레드 시작
        thread2.start(); // 두 번째 스레드 시작

        try {
            thread1.join(); // 첫 번째 스레드 종료 대기
            thread2.join(); // 두 번째 스레드 종료 대기
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        System.out.println("Final value of sharedData: " + sharedData);
    }

    static class Increment implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 100000; i++) {
                try {
                    semaphore.acquire(); // 세마포 획득
                    sharedData++;        // 공유 데이터 증가
                } catch (InterruptedException e) {
                     e.printStackTrace();
                } finally {
                    semaphore.release(); // 세마포 해제
                }
            }
             System.out.println("Increment 스레드 완료.");
        }
    }

    static class Decrement implements Runnable {
        @Override
        public void run() {
            for (int i = 0; i < 100000; i++) {
                try {
                    semaphore.acquire(); // 세마포 획득
                    sharedData--;        // 공유 데이터 감소
                } catch (InterruptedException e) {
                    e.printStackTrace();
                } finally {
                    semaphore.release(); // 세마포 해제
                }
            }
             System.out.println("Decrement 스레드 완료.");
        }
    }
}
```

---

### 조건 변수와 모니터

다음으로 알아볼 동기화 기법은 모니터이다. 

모니터를 이해하려면 우선 **조건 변수(Condition Variable)** 를 이해해야 한다.

#### 조건 변수 (Condition Variable)

- 목적
  - 스레드나 프로세스의 실행 순서를 제어하기 위한 동기화 도구이다. 세마포어가 자원의 개수를 세거나 신호를 보내는 데 중점을 둔다면, 조건 변수는 특정 조건이 만족될 때까지 기다리게 하는 데 특화되어 있다.
- 핵심 동작
  - `wait()`: 이 함수를 호출한 스레드는 특정 조건이 만족되기를 기다리며 대기 상태로 들어간다. (스스로 잠드는 것과 비슷)
  - `signal()`: 다른 스레드가 `wait()`를 호출하여 잠들어 있는 스레드에게 "이제 네가 기다리던 조건이 만족되었으니 깨어나도 돼!" 라고 알려주어 실행을 재개시키는 함수이다.
- 간단 요약:
  - 아직 원하는 조건이 안 됐으면? -> `wait()` 호출해서 기다린다.
  - 기다리던 조건이 충족됐으면? -> `signal()` 호출해서 기다리던 애를 깨운다.

다음은 조건 변수의 예시 코드이다.

```java
import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

public class CV {

    private static final Lock lock = new ReentrantLock();
    private static final Condition cond = lock.newCondition();
    private static boolean ready = false;

    public static void main(String[] args) throws InterruptedException {
        Thread t1 = new Thread(new ThreadJob1());
        Thread t2 = new Thread(new ThreadJob2());

        t1.start();
        t2.start();

        t1.join();
        t2.join();
    }

    static class ThreadJob1 implements Runnable {
        @Override
        public void run() {
            System.out.println("P1: 먼저 시작");
            lock.lock(); // 락 획득 (임계 구역 진입)
            try {
                System.out.println("P1: 2초 대기");
                // ready 변수가 true가 될 때까지 (조건이 만족될 때까지) 대기
                while (!ready) {
                    cond.await(); // 조건 변수 wait
                }
                // 이 아래 코드는 P2가 signal을 보낸 후에 실행됨
            } catch (InterruptedException e) {
                e.printStackTrace();
            } finally {
                lock.unlock(); // 락 해제
            }

            System.out.println("P1: 다시 시작");
            System.out.println("P1: 종료");
        }
    }

    static class ThreadJob2 implements Runnable {
        @Override
        public void run() {
            System.out.println("P2: 2초 실행 시작");
            try {
                Thread.sleep(2000); // 2초 대기
            } catch (InterruptedException e) {
                e.printStackTrace();
            }

            System.out.println("P2: 실행 완료");
            lock.lock(); // 락 획득 (임계 구역 진입)
            try {
                ready = true;   
                cond.signal();    // 조건 변수 signal (대기 중인 스레드 하나를 깨움)
            } finally {
                lock.unlock(); // 락 해제
            }
        }
    }
}
```

위 코드의 실행 결과는 다음과 같다.

```
P1: 먼저 시작
P1: 2초 대기
P2: 2초 실행 시작
P2: 실행 완료
P1: 다시 시작
P1: 종료
```

#### 모니터

모니터는 공유 자원과 그 공유 자원을 다루는 함수(인터페이스)들로 구성된 동기화 도구이다. 

모니터는 상호 배제(Mutual Exclusion) 를 위한 동기화뿐만 아니라, 

실행 순서 제어를 위한 동기화까지 가능하다는 특징을 가진다.

![모니터](./img/bsh_19.png)

#### 모니터의 상호 배제 원리:
모니터의 기본적인 작동 원리는 단순하다.
1. 프로세스 및 스레드는 공유 자원에 접근하기 위해 반드시 모니터가 제공하는 정해진 함수(인터페이스)를 통해서만 모니터 내부로 진입해야 한다.
2. 어떤 시점이든 모니터 안에 진입하여 코드를 실행하는 프로세스 및 스레드는 항상 하나여야 한다. (자동 상호 배제 보장)
3. 만약 이미 모니터 내부에 진입하여 실행 중인 프로세스/스레드가 있다면, 다른 프로세스/스레드가 모니터 내부로 진입하려고 할 때 자동으로 큐(Queue)에서 대기해야 한다.

#### 모니터의 실행 순서 제어
모니터는 앞서 설명된 조건 변수(Condition Variable) 를 함께 활용하여 실행 순서 제어를 위한 동기화도 구현할 수 있다.

예를 들어, 동시에 실행되는 프로세스 A와 B 중, 반드시 A가 먼저 실행되고 그 다음에 B가 실행되어야 한다는 조건을 가정해 보자.

1. **프로세스 B의 조건 확인 및 대기**
   - 프로세스 B가 먼저 모니터 내부에 진입했다고 가정하자. 
   - B는 실행하기 전에 "프로세스 A의 실행이 완료되었는지" 조건을 먼저 검사해야 한다. 
   - 만약 A가 아직 완료되지 않았다면, 이는 'B가 실행되어야 한다'는 조건이 아직 충족되지 않은 것이다. 
   - 이 경우, 프로세스 B는 모니터 내의 특정 조건 변수(cv)에 대해 cv.wait()를 호출하여 대기 상태로 들어간다. 
   - wait()를 호출하면 B는 모니터의 락(lock)을 임시로 해제하고 잠들게 되어, 다른 프로세스(여기서는 A)가 모니터에 진입할 수 있게 된다.

![프로세스 B 대기](./img/bsh_20.png)

2. **프로세스 A의 작업 완료 및 신호**
   - 프로세스 A가 모니터 내로 진입하여 자신의 작업을 완료한다. 
   - 작업 완료 후, A는 프로세스 B가 기다리고 있던 조건 변수(cv)에 대해 cv.signal()을 호출한다. 
   - 이 signal() 호출은 cv.wait()로 인해 잠들어 있던 프로세스 B를 깨우는 역할을 한다.
   
![프로세스 A 신호](./img/bsh_21.png)

3. **프로세스 B의 실행 재개**
   - signal() 호출로 깨어난 프로세스 B는 다시 모니터의 락을 얻으려고 시도한다. 
   - 락을 성공적으로 획득하면, 이전에 cv.wait()를 호출했던 지점 다음부터 실행을 재개한다.


결과적으로, 이 과정을 통해 반드시 프로세스 A가 먼저 실행되고, 그 다음 프로세스 B가 실행되는 순서 제어가 이루어진다.

모니터를 이용한 코드 예시는 다음과 같다.

```java
public synchronized void example(int value){
    // 이 메서드 전체가 임계 구역이 되며, 한 번에 하나의 스레드만 실행 가능
    this.count += value;
}
```

---

### 스레드 안전 (Thread Safety)

#### 스레드 안전이란
- 어떤 코드(함수, 클래스, 객체 등)가 여러 스레드로부터 동시에 접근되어 실행되더라도, 예상한 대로 올바르게 동작하고 데이터의 일관성이 깨지지 않는 상태 또는 성질을 의미한다.
- 즉, 멀티스레드 환경에서도 해당 코드를 안전하게 사용할 수 있음을 뜻한다.

#### 경쟁 상태
- 스레드 안전성은 경쟁 상태(Race Condition) 와 밀접한 관련이 있다. 
- 경쟁 상태는 여러 스레드가 동기화 없이 공유 자원에 접근하여 발생할 수 있는 문제 상황 그 자체를 의미한다.
- 스레드 안전성은 이러한 경쟁 상태를 방지하거나 적절히 제어하여 코드의 안정성을 확보한 상태이다. 
- 스레드 안전한 코드는 내부적으로 뮤텍스, 세마포어 등의 동기화 기법을 사용하여 공유 자원에 대한 접근을 조율함으로써 경쟁 상태를 막는다.

#### Java Vector vs ArrayList
스레드 안전성을 이해하는 좋은 예시가 Java의 Vector와 ArrayList이다. 

두 클래스 모두 리스트 형태의 데이터를 저장하지만, 스레드 안전성 측면에서 큰 차이가 있다.

- **Vector**
  - Vector 클래스의 대부분의 메서드(예: add, get, remove)는 synchronized 키워드로 선언되어 있다.
  - synchronized는 메서드 전체 또는 특정 코드 블록에 대해 자동으로 락(모니터 락)을 걸어주는 역할을 한다. 
  - 따라서 Vector 객체에 대해서는 한 번에 하나의 스레드만 add와 같은 메서드를 실행할 수 있다 (상호 배제 보장).
  - 이로 인해 Vector는 별도의 동기화 처리 없이도 기본적으로 스레드 안전(Thread-Safe) 하다. 
  - 여러 스레드가 동시에 add를 호출해도 경쟁 상태가 발생하지 않는다.
  - 단점: 모든 메서드 호출에 동기화 비용(락 획득/해제 오버헤드)이 발생하므로, 단일 스레드 환경이나 이미 외부에서 동기화를 처리하는 경우에는 성능 저하가 발생할 수 있다.

- **ArrayList**
  - ArrayList 클래스의 메서드(예: add, get, remove)는 synchronized로 선언되어 있지 않다.
  - 따라서 여러 스레드가 동시에 같은 ArrayList 객체의 add 메서드를 호출하면, 내부 데이터(배열, 크기 변수 등)에 대한 접근이 겹쳐 경쟁 상태가 발생할 수 있다. 이로 인해 리스트의 크기가 잘못 계산되거나, 데이터가 유실되거나, 예외가 발생하는 등의 문제가 생길 수 있다.
  - ArrayList는 기본적으로 스레드 안전하지 않다 (Not Thread-Safe). 멀티스레드 환경에서 안전하게 사용하려면, 개발자가 직접 외부에서 동기화 처리(예: Collections.synchronizedList() 래퍼 사용, CopyOnWriteArrayList 사용, 또는 직접 락 사용)를 해주어야 한다.
  - 장점: 동기화 오버헤드가 없으므로 단일 스레드 환경에서는 Vector보다 일반적으로 성능이 좋다.

---

## 교착 상태

![교착 상태](./img/bsh_22.png)

**교착 상태**란 두 개 이상의 프로세스가 서로 상대방이 가진 자원을 기다리며, 무한정 대기하여 더 이상 진행할 수 없는 상태이다.

### 교착 상태 조건

1. 상호 배제
2. 점유와 대기
3. 비선점
4. 원형 대기

### 해결 방법

#### 교착 상태 예방

- 교착 상태를 발생시키는 4가지 필요 조건 중 하나를 충족하지 못하게 하기

#### 교착 상태 회피

- 교착 상태를 한정된 자원의 무분별한 할당으로 인해 발생하는 문제로 간주함.
- 따라서 자원 할당 시 보수적으로 자원 할당하기

#### 교착 상태 검출 후 회복

- 프로세스를 자원 선점을 통해 회복
- 교착 상태에 놓인 프로세스를 강제 종료

<br/><br/>





# 4. CPU 스케줄링

## CPU 스케줄링 알고리즘

## 리눅스 CPU 스케줄링

<br/><br/>





# 5. 가상 메모리

## 물리 주소와 논리 주소

## 스와핑과 연속 메모리 할당

## 페이징을 통한 가상 메모리 관리

## 페이지 교체 알고리즘

<br/><br/>





# 6. 파일 시스템

## 파일과 디렉토리

## 파일 시스템

<br/><br/>
